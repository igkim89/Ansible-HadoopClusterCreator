bigdata:
  hosts:
    # Cluster all hosts
    s[1:3]:
      ansible_python_interpreter: "/home/{{ remote_user }}/.pyenv/shims/python"
  children:

    ## Worker Node
    #  (DataNode, NodeManager, RegionServer, Spark, NodeExporter가 설치될 Node)
    worker:
      hosts:
        s[1:3]:

    ## Scale-in (Decommission할 Worker Node, worker 목록에서 삭제 필수)
    scale-in:
      hosts:


    ## ZooKeeper Cluster
    zookeeper:
      hosts:
        s1:
          myid: 1
        s2:
          myid: 2
        s3:
          myid: 3

    ## Hadoop Cluster
    hadoop:
      hosts:
        s[1:3]:
      children:
        namenode:
          hosts:
            s[1:2]:
        # Use worker instead of datanode.
        datanode:
        journalnode:
          hosts:
            s[1:3]:
        resourcemanager:
          hosts:
            s[2:3]:
        # Use worker instead of nodemanager.
        nodemanager:
        jobhistoryserver:
          hosts:
            s1:
        httpfs:
          hosts:
            s1:

    ## Kafka Cluster
    kafka:
      hosts:
        s1:
          nodeid: 1
        s2:
          nodeid: 2
        s3:
          nodeid: 3

    ## pyenv
    # Hive metastore로 사용할 MariaDB 설치를 위한 pyenv 선행 설치
    pyenv:
      hosts:
        s[2:3]:

    ## MariaDB Server
    # pyenv 설치 필요
    mariadb:
      hosts:
        s3:
#          ansible_python_interpreter: "/home/{{ remote_user }}/.pyenv/shims/python"

    ## MySQL Server
    # pyenv 설치 필요
    mysql:
      hosts:
        s3:

    ## Hive Server
    # MariaDB 설치 필요
    hive:
      children:
        hive-server2:
          hosts:
            s[2:3]:
        metastore:
          hosts:
            s[2:3]:

    ## Spark
    # Use worker instead of spark.
    spark:
      children:
        spark-history:
          hosts:
            s3:

    ## HBase Cluster
    hbase:
      hosts:
      children:
        hbase-master:
          hosts:
            s[1:2]:
        # Use worker instead of region-server.
        region-server:

    ## Hue Server
    hue:
      hosts:
        s3:

    ## Sqoop client
    sqoop:
      hosts:
        s3:

    ## UI for Apache Kafka Web Server
    ufk:
      hosts:
        s3:

    ## Airflow
    airflow:
      hosts:
        s[1:3]:
      children:
        airflow-webserver:
          hosts:
            s3:
        airflow-scheduler:
          hosts:
            s[2:3]:
        airflow-celery-flower:
          hosts:
            s3:
        airflow-celery-worker:
          hosts:
            s[1:3]:

    ## Zeppelin
    zeppelin:
      hosts:
        s3:

    ## Redis Sentinel Cluster (3대로 구성)
    redis:
      hosts:
        s[1:3]:

    R:
      hosts:
        s3:

    prometheus:
      hosts:
        s3:

    # Use all hosts.
    node-exporter:

    blackbox-exporter:
      hosts:
        s3:

    check-process-exporter:
      hosts:
        s[1:3]:

    kminion-exporter:
      hosts:
        s3:

    grafana:
      hosts:
        s3:

    ranger:
      hosts:
        s3: