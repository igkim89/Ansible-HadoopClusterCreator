---
## Hadoop
- name: Install Hadoop cluster
  hosts: worker
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Create bigdata home directory
      become: yes
      file:
        dest: "{{ bigdata_home }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Copy installer
      copy:
        src: "{{ installer_home }}/hadoop/{{ installer.hadoop }}"
        dest: "{{ bigdata_home }}/{{ installer.hadoop }}"
    - name: Unzip installer
      unarchive:
        src: "{{ bigdata_home }}/{{ installer.hadoop }}"
        dest: "{{ bigdata_home }}/"
        remote_src: true
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
        creates: "{{ bigdata_home }}/hadoop"
    - name: Delete installer
      file:
        path: "{{ bigdata_home }}/{{ installer.hadoop }}"
        state: absent
    - name: Create hadoop home symbolic link
      file:
        src: "{{ bigdata_home }}/{{ installer.hadoop.replace('.tar.gz', '') }}"
        dest: "{{ bigdata_home }}/hadoop"
        state: link
    - name: Create configuration symbolic link
      file:
        src: "{{ bigdata_home }}/{{ installer.hadoop.replace('.tar.gz', '') }}/etc/hadoop"
        dest: "{{ bigdata_home }}/{{ installer.hadoop.replace('.tar.gz', '') }}/conf"
        state: link
    - name: Edit Java home
      lineinfile:
        path: "{{ bigdata_home }}/hadoop/conf/hadoop-env.sh"
        regexp: "# export JAVA_HOME="
        line: "export JAVA_HOME={{ java_home }}"
    - name: Edit HDFS_NAMENODE_OPTS
      lineinfile:
        path: "{{ bigdata_home }}/hadoop/conf/hadoop-env.sh"
        line: "{{ item }}"
        insertafter: EOF
      with_items:
        - 'export HDFS_NAMENODE_OPTS="-XX:+UseParallelGC -Xmx4g"'
        - 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native'
        - 'export HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native"'
    - name: Edit Hadoop home
      lineinfile:
        path: "/home/{{ remote_user }}/.bashrc"
        line: "{{ item }}"
        insertafter: EOF
      with_items:
        - "export HADOOP_HOME={{ bigdata_home }}/hadoop"
        - "PATH=$PATH:$HADOOP_HOME/bin"
        - "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native"
    - name: Refresh bash_profile
      shell: "source /home/{{ remote_user }}/.bashrc"
      args:
        executable: /bin/bash


- name: Create DataNode directory
  hosts: worker
  remote_user: "{{ remote_user }}"
  become: yes
  become_method: sudo
  become_user: root
  tasks:
    - name: Create DataNode directory
      file:
        path: "{{ item }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
      loop: "{{ hadoop.hdfs.datanode_dir }}"


- name: Create NodeManager log directory
  hosts: worker
  remote_user: "{{ remote_user }}"
  become: yes
  become_method: sudo
  become_user: root
  tasks:
    - name: Create DataNode directory
      file:
        path: "{{ item }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
      loop: "{{ hadoop.yarn.nodemanager_log_dir }}"


- name: Add configuration files
  hosts: worker
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hadoop_home: "{{ bigdata_home }}/hadoop"
  tasks:
    - name: Add Hadoop configuration files
      template:
        src: hadoop/{{ item }}
        dest: "{{ bigdata_home }}/hadoop/conf/"
        mode: 0644
      with_items:
        - "yarn-site.xml"
        - "mapred-site.xml"
        - "httpfs-site.xml"
        - "hdfs-site.xml"
        - "core-site.xml"
    - name: Create Nodemanager log dir
      file:
        path: "{{ hadoop_home }}/{{ item }}"
        state: directory
        mode: 0775
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
      with_items:
        - "logs"
        - "logs/userlogs"
    - name: Remove excludes file
      file:
        path: "{{ hadoop_home }}/conf/excludes"
        state: absent
    - name: Touch includes/excludes
      file:
        path: "{{ hadoop_home }}/conf/{{ item }}"
        state: touch
        mode: 0775
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
      with_items:
        - "includes"
        - "excludes"


- name: Start DataNode
  hosts: worker
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Check DataNode jps
      command: "/usr/bin/jps"
      register: datanode_jps
      failed_when: False
    - name: Start DataNode
      command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/hdfs --daemon start datanode"
      when: '"DataNode" not in datanode_jps.stdout'


- name: Start NodeManager
  hosts: worker
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Check NodeManager jps
      command: "/usr/bin/jps"
      register: nm_jps
      failed_when: False
    - command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/yarn --daemon start nodemanager"
      when: '"NodeManager" not in nm_jps.stdout'


- name: Refresh Nodes
  hosts: "{{ groups['namenode'][0] }}"
  become: no
  vars:
    hadoop_home: "{{ bigdata_home }}/hadoop"
  tasks:
    - name: Refresh Nodes
      command: "{{ hadoop_home }}/bin/hadoop dfsadmin -refreshNodes"


- name: Install Hive
  #  client 사용을 위해 클러스터 전체 배포
  hosts: worker
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hive_home: "{{ bigdata_home }}/hive"
    hive_spark_home: "{{ bigdata_home }}/{{ installer.hive_spark.replace('.tgz', '') }}"
  tasks:
    - name: Create bigdata home directory
      become: yes
      file:
        dest: "{{ bigdata_home }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Copy installer
      copy:
        src: "{{ installer_home }}/hive/{{ installer.hive }}"
        dest: "{{ bigdata_home }}/{{ installer.hive }}"
    - name: Unzip installer
      unarchive:
        src: "{{ bigdata_home }}/{{ installer.hive }}"
        dest: "{{ bigdata_home }}/"
        remote_src: true
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
        creates: "{{ hive_home }}"
    - name: Delete installer
      file:
        path: "{{ bigdata_home }}/{{ installer.hive }}"
        state: absent
    - name: Create Hive symbolic link
      file:
        src: "{{ bigdata_home }}/{{ installer.hive.replace('.tar.gz', '') }}"
        dest: "{{ hive_home }}"
        state: link
    - name: Copy Hive JDBC
      copy:
        src: "{{ installer_home }}/jdbc/{{ installer.mysql_jdbc }}"
        dest: "{{ hive_home }}/lib/"
    - name: Add hive configuration
      template:
        src: "hive/{{ item }}"
        dest: "{{ hive_home }}/conf/"
        mode: 0644
      with_items:
        - "hive-env.sh"
        - "hive-log4j2.properties"
        - "hive-site.xml"

    - name: Edit Hive home
      lineinfile:
        path: "/home/{{ remote_user }}/.bashrc"
        line: "{{ item }}"
        insertafter: EOF
      with_items:
        - "export HIVE_HOME={{ bigdata_home }}/hive"
        - "PATH=$PATH:$HIVE_HOME/bin"
    - name: Refresh bash_profile
      shell: "source /home/{{ remote_user }}/.bashrc"
      args:
        executable: /bin/bash
    - name: Add htrace library
      copy:
        src: "{{ installer_home }}/tez/{{ item }}"
        dest: "{{ hive_home }}/lib/"
      with_items:
        - "htrace-core4-4.1.0-incubating.jar"
    - name: Add hadoop classpath
      lineinfile:
        path: "{{ bigdata_home }}/hive/bin/hive"
        line: |
          for f in ${HADOOP_HOME}/share/hadoop/*/*.jar; do
            CLASSPATH=${CLASSPATH}:$f;
          done
        insertbefore: "# add the auxillary jars such as serdes"
    - name: Install Hive on Spark
      copy:
        src: "{{ installer_home }}/spark/{{ installer.hive_spark }}"
        dest: "{{ bigdata_home }}/{{ installer.hive_spark }}"
    - name: Unzip installer
      unarchive:
        src: "{{ bigdata_home }}/{{ installer.hive_spark }}"
        dest: "{{ bigdata_home }}/"
        remote_src: true
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
        creates: "{{ hive_spark_home }}"
    - name: Delete installer
      file:
        path: "{{ bigdata_home }}/{{ installer.hive_spark }}"
        state: absent
    - name: Link Spark library
      file:
        src: "{{ hive_spark_home }}/jars/{{ item }}"
        dest: "{{ hive_home }}/lib/{{ item }}"
        state: link
      with_items:
        - "scala-library-2.11.8.jar"
        - "spark-core_2.11-2.3.0.jar"
        - "spark-network-common_2.11-2.3.0.jar"
    - name: Add Spark configuration
      template:
        src: "hive/{{ item }}"
        dest: "{{ hive_spark_home }}/conf/"
        mode: 0644
      with_items:
        - "spark-defaults.conf"
        - "spark-env.sh"
    - name: Link spark-default.conf
      file:
        src: "{{ hive_spark_home }}/conf/{{ item }}"
        dest: "{{ hive_home }}/conf/{{ item }}"
        state: link
      with_items:
        - "spark-defaults.conf"
    - name: Add SKIP_HBASECP=true in Hive script
      lineinfile:
        path: "{{ hive_home }}/bin/hive"
        regexp: "^SKIP_HBASECP=false$"
        line: "SKIP_HBASECP=true"


## Spark
- name: Install Spark
  hosts: worker
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    spark_home: "{{ bigdata_home }}/spark"
  tasks:
    - name: Create bigdata home directory
      become: yes
      file:
        dest: "{{ bigdata_home }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Copy installer
      copy:
        src: "{{ installer_home }}/spark/{{ installer.spark }}"
        dest: "{{ bigdata_home }}/{{ installer.spark }}"
    - name: Unzip installer
      unarchive:
        src: "{{ bigdata_home }}/{{ installer.spark }}"
        dest: "{{ bigdata_home }}/"
        remote_src: true
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
        creates: "{{ spark_home }}"
    - name: Delete installer
      file:
        path: "{{ bigdata_home }}/{{ installer.spark }}"
        state: absent
    - name: Create Spark symbolic link
      file:
        src: "{{ bigdata_home }}/{{ installer.spark.replace('.tgz', '') }}"
        dest: "{{ spark_home }}"
        state: link
    - name: Edit Spark home and pyspark
      lineinfile:
        path: "/home/{{ remote_user }}/.bashrc"
        line: "{{ item }}"
        insertafter: EOF
      with_items:
        - "export SPARK_HOME={{ spark_home }}"
        - "PATH=$PATH:$SPARK_HOME/bin"
    - name: Edit pyspark
      lineinfile:
        path: "/home/{{ remote_user }}/.bashrc"
        regexp: "^#? *{{ item.key | regex_escape() }}="
        line: "{{ item.key }}={{ item.value }}"
      with_dict:
        "export PYSPARK_PYTHON": "\"$HOME/.pyenv/shims/python\""
    #        "export PYSPARK_DRIVER_PYTHON": python3
    - name: Add spark-env.sh
      template:
        src: spark/{{ item }}
        dest: "{{ spark_home }}/conf/"
        mode: 0644
      with_items:
        - spark-env.sh
        - spark-defaults.conf
    - name: Hive configuration symbolic link
      file:
        src: "{{ bigdata_home }}/hive/conf/hive-site.xml"
        dest: "{{ spark_home }}/conf/hive-site.xml"
        state: link
    - name: Add hive-hbase-handler and hive-common
      copy:
        src: "{{ installer_home }}/hive/{{ item }}"
        dest: "{{ spark_home }}/jars/"
      with_items:
        - "hive-hbase-handler-3.1.2-v2.jar"
        - "hive-common-2.3.9-v2.jar"


## HBase
- name: Install HBase
  hosts: worker
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hbase_home: "{{ bigdata_home }}/hbase"
  tasks:
    - name: Create bigdata home directory
      become: yes
      file:
        dest: "{{ bigdata_home }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Copy installer
      copy:
        src: "{{ installer_home }}/hbase/{{ installer.hbase }}"
        dest: "{{ bigdata_home }}/{{ installer.hbase }}"
    - name: Unzip installer
      unarchive:
        src: "{{ bigdata_home }}/{{ installer.hbase }}"
        dest: "{{ bigdata_home }}/"
        remote_src: true
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
        creates: "{{ hbase_home }}"
    - name: Delete installer
      file:
        path: "{{ bigdata_home }}/{{ installer.hbase }}"
        state: absent
    - name: Create hbase symbolic link
      file:
        src: "{{ bigdata_home }}/{{ installer.hbase.replace('-bin.tar.gz', '') }}"
        dest: "{{ hbase_home }}"
        state: link
    - name: Add hbase configuration
      template:
        src: "{{ item }}"
        dest: "{{ bigdata_home }}/hbase/conf/"
        mode: 0644
      with_fileglob:
        - "{{ ansible_home }}/playbooks/edit/templates/hbase/*"
    - name: Create tools directory
      file:
        dest: "{{ hbase_home }}/tools"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Copy HBase tools
      copy:
        src: "{{ installer_home }}/hbase/{{ installer.hbase_tools }}/{{ item }}"
        dest: "{{ hbase_home }}/tools/"
      with_items:
        - "hbase-hbck2-1.2.0.jar"
        - "hbase-table-reporter-1.2.0.jar"
        - "hbase-tools-1.2.0.jar"
    - name: Edit HBase classpath
      lineinfile:
        path: "/home/{{ remote_user }}/.bashrc"
        line: "{{ item }}"
        insertafter: EOF
      with_items:
        - "export HBASE_CLASSPATH=\"{{ hbase_home }}/tools/hbase-hbck2-1.2.0.jar:{{ hbase_home }}/tools/hbase-table-reporter-1.2.0.jar:{{ hbase_home }}/tools/hbase-tools-1.2.0.jar:$HBASE_CLASSPATH\""
    - name: Copy jmx_exporter
      copy:
        src: "{{ installer_home }}/prometheus/{{ installer.jmx_exporter }}"
        dest: "{{ hbase_home }}/tools/"
    - name: Edit Hadoop native library path (1/2)
      lineinfile:
        path: "{{ hbase_home }}/bin/hbase"
        insertafter: "org.apache.hadoop.hbase.util.GetJavaProperty java.library.path"
        line: "HADOOP_JAVA_LIBRARY_PATH=\"{{ bigdata_home }}/hadoop/lib/native\""
    - name: Edit Hadoop native library path (2/2)
      lineinfile:
        path: "{{ hbase_home }}/bin/hbase"
        search_string: "{{ item }}"
        line: "#{{ item }}"
      with_items:
        - 'HADOOP_JAVA_LIBRARY_PATH=$(HADOOP_CLASSPATH="$CLASSPATH${temporary_cp}" "${HADOOP_IN_PATH}" \'
        - 'org.apache.hadoop.hbase.util.GetJavaProperty java.library.path)'

    - name: Check HBase region-server jps
      command: "/usr/bin/jps"
      register: rs_jps
      failed_when: false
    - name: Start HBase region-server
      shell: "/usr/bin/nohup {{ hbase_home }}/bin/hbase-daemon.sh start regionserver"
      when: '"HRegionServer" not in rs_jps.stdout'


## NodeExporter
- name: Install NodeExporter
  hosts: worker
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    node_exporter_home: "{{ bigdata_home }}/node_exporter"
  tasks:
    - name: Copy installer
      copy:
        src: "{{ installer_home }}/prometheus/{{ installer.node_exporter }}"
        dest: "{{ bigdata_home }}/{{ installer.node_exporter }}"
    - name: Unzip installer
      unarchive:
        src: "{{ bigdata_home }}/{{ installer.node_exporter }}"
        dest: "{{ bigdata_home }}/"
        remote_src: true
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
        creates: "{{ node_exporter_home }}"
    - name: Delete installer
      file:
        path: "{{ bigdata_home }}/{{ installer.node_exporter }}"
        state: absent
    - name: Create Prometheus symbolic link
      file:
        src: "{{ bigdata_home }}/{{ installer.node_exporter.replace('.tar.gz', '') }}"
        dest: "{{ node_exporter_home }}"
        state: link
    - name: Create log directory
      file:
        path: "{{ node_exporter_home }}/logs"
        state: directory
    - name: Check NodeExporter PID
      shell: "/usr/bin/ps -ef | grep -v grep | grep node_exporter"
      register: ne_pid
      failed_when: false
    - name: Start NodeExporter
      shell: "/usr/bin/nohup {{ node_exporter_home }}/node_exporter > {{ node_exporter_home }}/logs/node_exporter.log 2>&1 &"
      when: '"node_exporter" not in ne_pid.stdout'


## Prometheus configuration 적용 및 재시작
- name: Edit Prometheus configuration
  hosts: prometheus
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    prometheus_home: "{{ bigdata_home }}/prometheus"
  tasks:
    - name: Edit Prometheus configuration
      template:
        src: "prometheus/{{ item }}"
        dest: "{{ bigdata_home }}/prometheus/"
        mode: 0644
      with_items:
        - "prometheus.yml"
    - name: Find Prometheus process ID
      shell: "/usr/bin/ps -ef | grep {{ prometheus_home }}/prometheus | grep -v grep | grep {{ prometheus.web_port }} | awk '{print $2}'"
      register: prometheus_pid
      failed_when: false
    - name: Kill Prometheus
      command: "/usr/bin/kill -9 {{ prometheus_pid.stdout }}"
      when: prometheus_pid.stdout !=""
    - name: Check Prometheus process
      shell: "/usr/bin/ps -ef | grep -v grep | grep {{ prometheus_home }}/prometheus | grep {{ prometheus.web_port }}"
      register: prometheus_ps
      failed_when: false
    - name: Start Prometheus
      shell: "/usr/bin/nohup {{ prometheus_home }}/prometheus --config.file={{ prometheus_home }}/prometheus.yml --web.listen-address=0.0.0.0:{{ prometheus.web_port }} > {{ prometheus_home }}/logs/prometheus.log 2>&1 &"
      when: '"prometheus" not in prometheus_ps.stdout'
