---

- name: Install Hadoop cluster
  hosts: bigdata
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Create bigdata home directory
      become: yes
      file:
        dest: "{{ bigdata_home }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Copy installer
      copy:
        src: "{{ installer_home }}/hadoop/{{ installer.hadoop }}"
        dest: "{{ bigdata_home }}/{{ installer.hadoop }}"
    - name: Unzip installer
      unarchive:
        src: "{{ bigdata_home }}/{{ installer.hadoop }}"
        dest: "{{ bigdata_home }}/"
        remote_src: true
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Delete installer
      file:
        path: "{{ bigdata_home }}/{{ installer.hadoop }}"
        state: absent
    - name: Create hadoop home symbolic link
      file:
        src: "{{ bigdata_home }}/{{ installer.hadoop.replace('.tar.gz', '') }}"
        dest: "{{ bigdata_home }}/hadoop"
        state: link
    - name: Create configuration symbolic link
      file:
        src: "{{ bigdata_home }}/{{ installer.hadoop.replace('.tar.gz', '') }}/etc/hadoop"
        dest: "{{ bigdata_home }}/{{ installer.hadoop.replace('.tar.gz', '') }}/conf"
        state: link
    - name: 디버깅
      debug:
        msg: "debugging.."
    - name: Edit Java home
      lineinfile:
        path: "{{ bigdata_home }}/hadoop/conf/hadoop-env.sh"
        regexp: "# export JAVA_HOME="
        line: "export JAVA_HOME={{ java_home }}"
    - name: Edit HDFS_NAMENODE_OPTS
      lineinfile:
        path: "{{ bigdata_home }}/hadoop/conf/hadoop-env.sh"
        line: "{{ item }}"
        insertafter: EOF
      with_items:
        - 'export HDFS_NAMENODE_OPTS="-XX:+UseParallelGC -Xmx4g"'
        - 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native'
        - 'export HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native"'
    - name: Edit Hadoop home
      lineinfile:
        path: "/home/{{ remote_user }}/.bashrc"
        line: "{{ item }}"
        insertafter: EOF
      with_items:
        - "export HADOOP_HOME={{ bigdata_home }}/hadoop"
        - "PATH=$PATH:$HADOOP_HOME/bin"
        - "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native"
    - name: Refresh bash_profile
      shell: "source /home/{{ remote_user }}/.bashrc"
      args:
        executable: /bin/bash

- name: Create NameNode directory
  hosts: namenode
  remote_user: "{{ remote_user }}"
  become: yes
  become_method: sudo
  become_user: root
  tasks:
    - name: Create NameNode directory
      file:
        path: "{{ item }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
      loop: "{{ hadoop.hdfs.namenode_dir }}"

- name: Create DataNode directory
  hosts: datanode
  remote_user: "{{ remote_user }}"
  become: yes
  become_method: sudo
  become_user: root
  tasks:
    - name: Create DataNode directory
      file:
        path: "{{ item }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
      loop: "{{ hadoop.hdfs.datanode_dir }}"

- name: Create JournalNode directory
  hosts: journalnode
  remote_user: "{{ remote_user }}"
  become: yes
  become_method: sudo
  become_user: root
  tasks:
    - name: Create JournalNode directory
      file:
        path: "{{ hadoop.hdfs.journalnode_dir }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"

- name: Add configuration files
  hosts: hadoop
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hadoop_home: "{{ bigdata_home }}/hadoop"
  tasks:
    - name: Add Hadoop configuration files
      template:
        src: hadoop/{{ item }}
        dest: "{{ bigdata_home }}/hadoop/conf/"
        mode: 0644
      with_items:
        - "yarn-site.xml"
        - "mapred-site.xml"
        - "httpfs-site.xml"
        - "hdfs-site.xml"
        - "core-site.xml"
    - name: Create Nodemanager log dir
      file:
        path: "{{ hadoop_home }}/{{ item }}"
        state: directory
        mode: 0775
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
      with_items:
        - "logs"
        - "logs/userlogs"

- name: Start JournalNode
  hosts: journalnode
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Check journalnode jps
      command: "/usr/bin/jps"
      register: journalnode_jps
      changed_when: False
    - name: Start Journalnode
      command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/hdfs --daemon start journalnode &"
      when: '"JournalNode" not in journalnode_jps.stdout'

- name: Setup Active NameNode
  hosts: "{{ groups['namenode'][0] }}"
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Zookeeper failover controller format
      command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/hdfs zkfc -formatZK"
    - name: NameNode format
      command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/hdfs namenode -format"

- name: Start NameNode, JournalNode, ZKFC
  hosts: "{{ groups['namenode'][0] }}"
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/sbin/start-dfs.sh"

- name: Setup Standby NameNode
  hosts: "{{ groups['namenode'][1] }}"
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Setup bootstrapStandby
      command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/hdfs namenode -bootstrapStandby"
    - name: Start Standby NameNode
      command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/hdfs --daemon start namenode"

- name: Start DataNode
  hosts: datanode
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Check DataNode serivce
      command: "/usr/bin/jps"
      register: datanode_jps
      changed_when: False
    - name: Start DataNode
      command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/hdfs --daemon start datanode"
      when: '"DataNode" not in datanode_jps.stdout'

- name: Start ResourceManager
  hosts: resourcemanager
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/yarn --daemon start resourcemanager"

- name: Start NodeManager
  hosts: nodemanager
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/yarn --daemon start nodemanager"

- name: Start JobHistory Server
  hosts: jobhistoryserver
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/mapred --daemon start historyserver"

- name: Start httpfs
  hosts: "{{ groups['namenode'][0] }}"
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/hdfs --daemon start httpfs"

## 계정별 디렉토리 생성
- name: Create user home directory
  hosts: "{{ groups['namenode'][0] }}"
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hadoop_home: "{{ bigdata_home }}/hadoop"
  tasks:
    - name: Create user home directory
      command: "{{ hadoop_home }}/bin/hdfs dfs -mkdir -p /user/{{ item }}"
      with_items:
        - "{{ remote_user }}"
        - "data_raw"
        - "data_parsed"
        - "dbsync"
        - "spark_streaming"
    - name: Change owner
      command: "{{ hadoop_home }}/bin/hdfs dfs -chown {{ item }}:{{ remote_user }} /user/{{ item }}"
      with_items:
        - "{{ remote_user }}"
        - "data_raw"
        - "data_parsed"
        - "dbsync"
        - "spark_streaming"