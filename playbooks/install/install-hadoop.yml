---

- name: Install Hadoop cluster
  hosts: bigdata
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Create bigdata home directory
      become: yes
      file:
        dest: "{{ bigdata_home }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Copy installer
      copy:
        src: "{{ installer_home }}/hadoop/{{ installer.hadoop }}"
        dest: "{{ bigdata_home }}/{{ installer.hadoop }}"
    - name: Unzip installer
      unarchive:
        src: "{{ bigdata_home }}/{{ installer.hadoop }}"
        dest: "{{ bigdata_home }}/"
        remote_src: true
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
        creates: "{{ bigdata_home }}/hadoop"
    - name: Delete installer
      file:
        path: "{{ bigdata_home }}/{{ installer.hadoop }}"
        state: absent
    - name: Create hadoop home symbolic link
      file:
        src: "{{ bigdata_home }}/{{ installer.hadoop.replace('.tar.gz', '') }}"
        dest: "{{ bigdata_home }}/hadoop"
        state: link
    - name: Create configuration symbolic link
      file:
        src: "{{ bigdata_home }}/{{ installer.hadoop.replace('.tar.gz', '') }}/etc/hadoop"
        dest: "{{ bigdata_home }}/{{ installer.hadoop.replace('.tar.gz', '') }}/conf"
        state: link
    - name: Edit Java home
      lineinfile:
        path: "{{ bigdata_home }}/hadoop/conf/hadoop-env.sh"
        regexp: "# export JAVA_HOME="
        line: "export JAVA_HOME={{ java_home }}"
    - name: Edit HDFS_NAMENODE_OPTS
      lineinfile:
        path: "{{ bigdata_home }}/hadoop/conf/hadoop-env.sh"
        line: "{{ item }}"
        insertafter: EOF
      with_items:
        - 'export HDFS_NAMENODE_OPTS="-XX:+UseParallelGC -Xmx4g"'
        - 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native'
        - 'export HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native"'
    - name: Edit Hadoop home
      lineinfile:
        path: "/home/{{ remote_user }}/.bashrc"
        line: "{{ item }}"
        insertafter: EOF
      with_items:
        - "export HADOOP_HOME={{ bigdata_home }}/hadoop"
        - "PATH=$PATH:$HADOOP_HOME/bin"
        - "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native"
    - name: Refresh bash_profile
      shell: "source /home/{{ remote_user }}/.bashrc"
      args:
        executable: /bin/bash

- name: Create NameNode directory
  hosts: namenode
  remote_user: "{{ remote_user }}"
  become: yes
  become_method: sudo
  become_user: root
  tasks:
    - name: Create NameNode directory
      file:
        path: "{{ item }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
      loop: "{{ hadoop.hdfs.namenode_dir }}"

- name: Create DataNode directory
  hosts: datanode, worker
  remote_user: "{{ remote_user }}"
  become: yes
  become_method: sudo
  become_user: root
  tasks:
    - name: Create DataNode directory
      file:
        path: "{{ item }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
      loop: "{{ hadoop.hdfs.datanode_dir }}"

- name: Create NodeManager log directory
  hosts: nodemanager
  remote_user: "{{ remote_user }}"
  become: yes
  become_method: sudo
  become_user: root
  tasks:
    - name: Create DataNode directory
      file:
        path: "{{ item }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
      loop: "{{ hadoop.yarn.nodemanager_log_dir }}"

- name: Create JournalNode directory
  hosts: journalnode
  remote_user: "{{ remote_user }}"
  become: yes
  become_method: sudo
  become_user: root
  tasks:
    - name: Create JournalNode directory
      file:
        path: "{{ hadoop.hdfs.journalnode_dir }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"

- name: Add configuration files
  hosts: hadoop
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hadoop_home: "{{ bigdata_home }}/hadoop"
  tasks:
    - name: Add Hadoop configuration files
      template:
        src: hadoop/{{ item }}
        dest: "{{ bigdata_home }}/hadoop/conf/"
        mode: 0644
      with_items:
        - "yarn-site.xml"
        - "mapred-site.xml"
        - "httpfs-site.xml"
        - "hdfs-site.xml"
        - "core-site.xml"
    - name: Check Ranger plugin status
      shell: "ls -l {{ hadoop_home }}/conf | grep ranger-security.xml"
      register: rp_stat
      failed_when: false
    - name: Add Ranger plugin configuration
      lineinfile:
        path: "{{ hadoop_home }}/conf/hdfs-site.xml"
        line: |
              <property>
                  <name>dfs.permissions.enabled</name>
                  <value>true</value>
              </property>
              <property>
                  <name>dfs.permissions</name>
                  <value>true</value>
              </property>
              <property>
                  <name>dfs.namenode.inode.attributes.provider.class</name>
                  <value>org.apache.ranger.authorization.hadoop.RangerHdfsAuthorizer</value>
              </property>
              <property>
                  <name>dfs.permissions.ContentSummary.subAccess</name>
                  <value>true</value>
              </property>
        insertbefore: "</configuration>"
      when: '"ranger-security.xml" in rp_stat.stdout'
    - name: Create Nodemanager log dir
      file:
        path: "{{ hadoop_home }}/{{ item }}"
        state: directory
        mode: 0775
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
      with_items:
        - "logs"
        - "logs/userlogs"
    - name: Touch includes/excludes
      file:
        path: "{{ hadoop_home }}/conf/{{ item }}"
        state: touch
        mode: 0775
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
      with_items:
        - "includes"
        - "excludes"


- name: Start JournalNode
  hosts: journalnode
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Check JournalNode jps
      command: "/usr/bin/jps"
      register: journalnode_jps
      failed_when: false
    - name: Start Journalnode
      command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/hdfs --daemon start journalnode &"
      when: '"JournalNode" not in journalnode_jps.stdout'

- name: Setup Active NameNode
  hosts: "{{ groups['namenode'][0] }}"
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Check already format (1/2)
      stat:
        path: "{{ hadoop.hdfs.namenode_dir[0] }}/current/VERSION"
      register: version_result
    - name: Check already format (2/2)
      stat:
        path: "{{ hadoop.hdfs.namenode_dir[0] }}/in_use.lock"
      register: lock_result
    - name: Zookeeper failover controller format
      command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/hdfs zkfc -formatZK"
      when: >
        (not version_result.stat.exists)
        and
        (not lock_result.stat.exists)
    - name: NameNode format
      command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/hdfs namenode -format -force"
      when: >
        (not version_result.stat.exists)
        and
        (not lock_result.stat.exists)

- name: Start NameNode, JournalNode, ZKFC
  hosts: "{{ groups['namenode'][0] }}"
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/sbin/start-dfs.sh"

- name: Setup Standby NameNode
  hosts: "{{ groups['namenode'][1] }}"
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Check already format (1/2)
      stat:
        path: "{{ hadoop.hdfs.namenode_dir[0] }}/current/VERSION"
      register: version_result
    - name: Check already format (2/2)
      stat:
        path: "{{ hadoop.hdfs.namenode_dir[0] }}/in_use.lock"
      register: lock_result
    - name: Setup bootstrapStandby
      command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/hdfs namenode -bootstrapStandby"
      when: >
        (not version_result.stat.exists)
        and
        (not lock_result.stat.exists)
    - name: Check NameNode jps
      command: "/usr/bin/jps"
      register: namenode_jps
      failed_when: False
    - name: Start Standby NameNode
      command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/hdfs --daemon start namenode"
      when: '"NameNode" not in namenode_jps.stdout'

- name: Start DataNode
  hosts: datanode, worker
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Check DataNode jps
      command: "/usr/bin/jps"
      register: datanode_jps
      failed_when: False
    - name: Start DataNode
      command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/hdfs --daemon start datanode"
      when: '"DataNode" not in datanode_jps.stdout'

- name: Start ResourceManager
  hosts: resourcemanager
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Check ResourceManager jps
      command: "/usr/bin/jps"
      register: rm_jps
      changed_when: False
    - command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/yarn --daemon start resourcemanager"
      when: '"ResourceManager" not in rm_jps.stdout'

- name: Start NodeManager
  hosts: nodemanager, worker
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Check NodeManager jps
      command: "/usr/bin/jps"
      register: nm_jps
      failed_when: False
    - command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/yarn --daemon start nodemanager"
      when: '"NodeManager" not in nm_jps.stdout'

- name: Start JobHistory Server
  hosts: jobhistoryserver
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Check JobHistory Server jps
      command: "/usr/bin/jps"
      register: hs_jps
      failed_when: False
    - command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/mapred --daemon start historyserver"
      when: '"JobHistoryServer" not in hs_jps.stdout'

- name: Start httpfs
  hosts: httpfs
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Check HttpFSServer jps
      command: "/usr/bin/jps"
      register: httpfs_jps
      failed_when: False
    - command: "/usr/bin/nohup {{ bigdata_home }}/hadoop/bin/hdfs --daemon start httpfs"
      when: '"HttpFSServerWebServer" not in httpfs_jps.stdout'

## 계정별 디렉토리 생성
- name: Create user home directory
  hosts: "{{ groups['namenode'][0] }}"
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hadoop_home: "{{ bigdata_home }}/hadoop"
  tasks:
    - name: Create user home directory
      command: "{{ hadoop_home }}/bin/hdfs dfs -mkdir -p /user/{{ item }}"
      with_items:
        - "{{ remote_user }}"
        - "data_raw"
        - "data_parsed"
        - "dbsync"
        - "spark_streaming"
    - name: Change owner
      command: "{{ hadoop_home }}/bin/hdfs dfs -chown {{ item }}:{{ remote_user }} /user/{{ item }}"
      with_items:
        - "{{ remote_user }}"
        - "data_raw"
        - "data_parsed"
        - "dbsync"
        - "spark_streaming"