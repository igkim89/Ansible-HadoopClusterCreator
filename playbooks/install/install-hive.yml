---

- name: Install Hive
#  client 사용을 위해 클러스터 전체 배포
  hosts: bigdata
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hive_home: "{{ bigdata_home }}/hive"
    hive_spark_home: "{{ bigdata_home }}/{{ installer.hive_spark.replace('.tgz', '') }}"
  tasks:
    - name: Create bigdata home directory
      become: yes
      file:
        dest: "{{ bigdata_home }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Copy installer
      copy:
        src: "{{ installer_home }}/hive/{{ installer.hive }}"
        dest: "{{ bigdata_home }}/{{ installer.hive }}"
    - name: Unzip installer
      unarchive:
        src: "{{ bigdata_home }}/{{ installer.hive }}"
        dest: "{{ bigdata_home }}/"
        remote_src: true
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
        creates: "{{ hive_home }}"
    - name: Delete installer
      file:
        path: "{{ bigdata_home }}/{{ installer.hive }}"
        state: absent
    - name: Create Hive symbolic link
      file:
        src: "{{ bigdata_home }}/{{ installer.hive.replace('.tar.gz', '') }}"
        dest: "{{ hive_home }}"
        state: link
    - name: Copy Hive JDBC
      copy:
        src: "{{ installer_home }}/jdbc/{{ installer.mysql_jdbc }}"
        dest: "{{ hive_home }}/lib/"
    - name: Add hive configuration
      template:
        src: "hive/{{ item }}"
        dest: "{{ hive_home }}/conf/"
        mode: 0644
      with_items:
        - "hive-env.sh"
        - "hive-log4j2.properties"
        - "hive-site.xml"

    - name: Edit Hive home
      lineinfile:
        path: "/home/{{ remote_user }}/.bashrc"
        line: "{{ item }}"
        insertafter: EOF
      with_items:
        - "export HIVE_HOME={{ bigdata_home }}/hive"
        - "PATH=$PATH:$HIVE_HOME/bin"
    - name: Refresh bash_profile
      shell: "source /home/{{ remote_user }}/.bashrc"
      args:
        executable: /bin/bash
    - name: Add htrace library
      copy:
        src: "{{ installer_home }}/tez/{{ item }}"
        dest: "{{ hive_home }}/lib/"
      with_items:
        - "htrace-core4-4.1.0-incubating.jar"
    - name: Add hadoop classpath
      lineinfile:
        path: "{{ bigdata_home }}/hive/bin/hive"
        line: |
          for f in ${HADOOP_HOME}/share/hadoop/*/*.jar; do
            CLASSPATH=${CLASSPATH}:$f;
          done
        insertbefore: "# add the auxillary jars such as serdes"
    - name: Install Hive on Spark
      copy:
        src: "{{ installer_home }}/spark/{{ installer.hive_spark }}"
        dest: "{{ bigdata_home }}/{{ installer.hive_spark }}"
    - name: Unzip installer
      unarchive:
        src: "{{ bigdata_home }}/{{ installer.hive_spark }}"
        dest: "{{ bigdata_home }}/"
        remote_src: true
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
        creates: "{{ hive_spark_home }}"
    - name: Delete installer
      file:
        path: "{{ bigdata_home }}/{{ installer.hive_spark }}"
        state: absent
    - name: Link Spark library
      file:
        src: "{{ hive_spark_home }}/jars/{{ item }}"
        dest: "{{ hive_home }}/lib/{{ item }}"
        state: link
      with_items:
        - "scala-library-2.11.8.jar"
        - "spark-core_2.11-2.3.0.jar"
        - "spark-network-common_2.11-2.3.0.jar"
    - name: Add Spark configuration
      template:
        src: "hive/{{ item }}"
        dest: "{{ hive_spark_home }}/conf/"
        mode: 0644
      with_items:
        - "spark-defaults.conf"
        - "spark-env.sh"
    - name: Link spark-default.conf
      file:
        src: "{{ hive_spark_home }}/conf/{{ item }}"
        dest: "{{ hive_home }}/conf/{{ item }}"
        state: link
      with_items:
        - "spark-defaults.conf"
    - name: Add SKIP_HBASECP=true in Hive script
      lineinfile:
        path: "{{ hive_home }}/bin/hive"
        regexp: "^SKIP_HBASECP=false$"
        line: "SKIP_HBASECP=true"


- name: Create hive metastore database
  hosts: mysql
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hive_home: "{{ bigdata_home }}/hive"
  tasks:
    - name: Create Hive metastore database
      command: "{{ mysql.home_dir }}/bin/mysql --socket={{ mysql.socket_dir }}/mysql.sock -uroot -p{{ mysql.root_pw }} -e 'CREATE DATABASE IF NOT EXISTS metastore;'"
      args:
        chdir: "{{ mysql.home_dir }}"
    - name: Create metastore user
      shell: |
        bin/mysql --socket=mysql.sock -uroot -p{{ mysql.root_pw }} -e "CREATE USER IF NOT EXISTS 'hive'@'{{ item }}' IDENTIFIED BY '{{ hive.metastore.db_pw }}';"
        bin/mysql --socket=mysql.sock -uroot -p{{ mysql.root_pw }} -e "REVOKE ALL PRIVILEGES, GRANT OPTION FROM 'hive'@'{{ item }}';"
        bin/mysql --socket=mysql.sock -uroot -p{{ mysql.root_pw }} -e "GRANT ALL PRIVILEGES ON metastore.* TO 'hive'@'{{ item }}';"
        bin/mysql --socket=mysql.sock -uroot -p{{ mysql.root_pw }} -e "FLUSH PRIVILEGES;"
      args:
        chdir: "{{ mysql.home_dir }}"
      with_items: "{{ groups['metastore'] }}"

- name: Create HDFS directory
  hosts: "{{ groups['namenode'][0] }}"
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hadoop_home: "{{ bigdata_home }}/hadoop"
    hive_spark_home: "{{ bigdata_home }}/{{ installer.hive_spark.replace('.tgz', '') }}"
  tasks:
    - name: Create Hive warehouse directory
      command: "{{ item }}"
      with_items:
        - "{{ hadoop_home }}/bin/hdfs dfs -mkdir /tmp"
        - "{{ hadoop_home }}/bin/hdfs dfs -mkdir /tmp/hive"
        - "{{ hadoop_home }}/bin/hdfs dfs -mkdir -p /user/hive/warehouse"
        - "{{ hadoop_home }}/bin/hdfs dfs -chmod 777 /tmp"
        - "{{ hadoop_home }}/bin/hdfs dfs -chmod 777 /tmp/hive"
        - "{{ hadoop_home }}/bin/hdfs dfs -chmod g+w /user/hive/warehouse"
      register: ware_result
      failed_when: >
        ("File exists" not in ware_result.stderr) and
        (ware_result.stderr != "")
    - name: Upload Spark library
      shell: "{{ item }}"
      with_items:
        - "{{ hadoop_home }}/bin/hdfs dfs -mkdir -p /hive-on-spark/archive"
        - "{{ hadoop_home }}/bin/hdfs dfs -put {{ hive_spark_home }}/jars/* /hive-on-spark/archive/"
      register: lib_result
      failed_when: >
        ("File exists" not in lib_result.stderr) and
        (lib_result.stderr != "")
    - name: Create Spark log directory
      command: "{{ hadoop_home }}/bin/hdfs dfs -mkdir /spark-logs"
      register: log_result
      failed_when: >
        ("File exists" not in log_result.stderr) and
        (log_result.stderr != "")

############################################
#  2023. 03. 10.
#  Hadoop 3.3.4 사용시 Tez 0.10.2 연동되었으나
#  Hadoop 3.2.4 변경 후 의존성 충돌로 인해 연동 불가
#  Hive on Spark로 변경
############################################
#- name: Install Tez
#  hosts: hive-server2,nodemanager
#  remote_user: "{{ remote_user }}"
#  become: no
#  vars:
#    tez_home: "{{ bigdata_home }}/tez"
#  tasks:
#    - name: Copy installer
#      copy:
#        src: "{{ installer_home }}/tez/{{ installer.tez }}"
#        dest: "{{ bigdata_home }}/{{ installer.tez }}"
#    - name: Create Tez home directory
#      file:
#        path: "{{ bigdata_home }}/{{ installer.tez.replace('.tar.gz', '') }}"
#        state: directory
#    - name: Unzip installer
#      unarchive:
#        src: "{{ bigdata_home }}/{{ installer.tez }}"
#        dest: "{{ bigdata_home }}/{{ installer.tez.replace('.tar.gz', '') }}/"
#        remote_src: true
#        owner: "{{ remote_user }}"
#        group: "{{ remote_user }}"
#    - name: Delete installer
#      file:
#        path: "{{ bigdata_home }}/{{ installer.tez }}"
#        state: absent
#    - name: Create Tez symbolic link
#      file:
#        src: "{{ bigdata_home }}/{{ installer.tez.replace('.tar.gz', '') }}"
#        dest: "{{ tez_home }}"
#        state: link
#    - name: Create configuration directory
#      file:
#        path: "{{ tez_home }}/conf"
#        state: directory
#    - name: Add Tez configuration
#      template:
#        src: tez/{{ item }}
#        dest: "{{ tez_home }}/conf/"
#      with_items:
#        - tez-site.xml
#    - name: Add library
#      copy:
#        src: "{{ installer_home }}/tez/{{ item }}"
#        dest: "{{ tez_home }}/lib/"
#      with_items:
#        - "hadoop-common-3.3.1.jar" # 3.3.4는 왜 안될까..? snappy 변경 때문인듯..?
#        - "htrace-core4-4.1.0-incubating.jar"

#- name: Upload Tez library
#  hosts: "{{ groups['datanode'][0] }}"
#  remote_user: "{{ remote_user }}"
#  become: no
#  tasks:
#    - name: Create library directory
#      command: "{{ bigdata_home }}/hadoop/bin/hdfs dfs -mkdir -p {{ tez.lib_dir }}"
#    - name: Check library
#      command: "{{ bigdata_home }}/hadoop/bin/hdfs dfs -ls {{ tez.lib_dir }}"
#      register: check_tez_lib
#    - name: Upload library
#      command: "{{ bigdata_home }}/hadoop/bin/hdfs dfs -put {{ installer_home }}/tez/{{ installer.tez }} {{ tez.lib_dir }}/"
#      when: 'installer.tez not in check_tez_lib.stdout'

- name: Check metastore table
  hosts: mysql
  remote_user: "{{ remote_user }}"
  become: no
  tasks:
    - name: Check metastore table
      command: "{{ mysql.home_dir }}/bin/mysql --socket={{ mysql.socket_dir }}/mysql.sock -uroot -p{{ mysql.root_pw }} -e 'SHOW TABLES FROM metastore;'"
      args:
        chdir: "{{ mysql.home_dir }}"
      register: meta_result
      failed_when: false
    - name: Add host
      add_host:
        name: meta
        meta_result: "{{ meta_result }}"


- name: Init schema Metastore
  hosts: "{{ groups['metastore'][0] }}"
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hive_home: "{{ bigdata_home }}/hive"
    meta_result: "{{ hostvars['meta']['meta_result'] }}"
  tasks:
    - name: Init schema
      shell: "{{ hive_home }}/bin/schematool --dbType mysql -initSchema"
      when: >
        ("Unknown database" in meta_result.stderr)
        or
        ("Tables_in_metastore" not in meta_result.stdout)
      failed_when: false


- name: Edit HCatalog configuration
  hosts: metastore
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hive_home: "{{ bigdata_home }}/hive"
  tasks:
    - name: Edit log path
      lineinfile:
        path: "{{ hive_home }}/hcatalog/sbin/hcat_server.sh"
        search_string: "HCAT_LOG_DIR=${HCAT_LOG_DIR:-\"$bin\"/../var/log}"
        line: "HCAT_LOG_DIR=${HIVE_HOME}/logs"
    - name: Create log directory
      file:
        path: "{{ hive_home }}/logs"
        state: directory
    - name: Edit metastore port
      lineinfile:
        path: "{{ hive_home }}/hcatalog/sbin/hcat_server.sh"
        search_string: "export METASTORE_PORT=${METASTORE_PORT:-9083}"
        line: "export METASTORE_PORT=${METASTORE_PORT:-{{ hive.metastore.metastore_port }}}"
    - name: Check HCtalog process
      shell: "/usr/bin/ps -ef | grep -v grep | grep 'hive.metastore.HiveMetaStore'"
      register: meta_ps
      failed_when: false
    - name: Start HCatalog server
      command: "{{ hive_home }}/hcatalog/sbin/hcat_server.sh start"
      when: '"hive.metastore.HiveMetaStore" not in meta_ps.stdout'

- name: Start HiveServer2
  hosts: hive-server2
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hive_home: "{{ bigdata_home }}/hive"
  tasks:
    - name: Check HiveServer2 process
      shell: "/usr/bin/ps -ef | grep -v grep | grep 'hive.service.server.HiveServer2'"
      register: hs_ps
      failed_when: false
    - name: Start HiveServer2
      shell: "/usr/bin/nohup {{ hive_home }}/bin/hiveserver2 > /dev/null 2>&1 &"
      when: '"hive.service.server.HiveServer2" not in hs_ps.stdout'





