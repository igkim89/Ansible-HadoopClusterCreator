---

- name: Install Spark
  hosts: bigdata
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    spark_home: "{{ bigdata_home }}/spark"
  tasks:
    - name: Create bigdata home directory
      become: yes
      file:
        dest: "{{ bigdata_home }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Copy installer
      copy:
        src: "{{ installer_home }}/spark/{{ installer.spark }}"
        dest: "{{ bigdata_home }}/{{ installer.spark }}"
    - name: Unzip installer
      unarchive:
        src: "{{ bigdata_home }}/{{ installer.spark }}"
        dest: "{{ bigdata_home }}/"
        remote_src: true
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Delete installer
      file:
        path: "{{ bigdata_home }}/{{ installer.spark }}"
        state: absent
    - name: Create Spark symbolic link
      file:
        src: "{{ bigdata_home }}/{{ installer.spark.replace('.tgz', '') }}"
        dest: "{{ spark_home }}"
        state: link
    - name: Edit Spark home and pyspark
      lineinfile:
        path: "/home/{{ remote_user }}/.bashrc"
        line: "{{ item }}"
        insertafter: EOF
      with_items:
        - "export SPARK_HOME={{ spark_home }}"
        - "PATH=$PATH:$SPARK_HOME/bin"
    - name: Edit pyspark
      lineinfile:
        path: "/home/{{ remote_user }}/.bashrc"
        regexp: "^#? *{{ item.key | regex_escape() }}="
        line: "{{ item.key }}={{ item.value }}"
      with_dict:
        "export PYSPARK_PYTHON": "\"$HOME/.pyenv/shims/python\""
#        "export PYSPARK_DRIVER_PYTHON": python3
    - name: Add spark-env.sh
      template:
        src: spark/{{ item }}
        dest: "{{ spark_home }}/conf/"
        mode: 0644
      with_items:
        - spark-env.sh
        - spark-defaults.conf
    - name: Hive configuration symbolic link
      file:
        src: "{{ bigdata_home }}/hive/conf/hive-site.xml"
        dest: "{{ spark_home }}/conf/hive-site.xml"
        state: link
    - name: Add hive-hbase-handler
      copy:
        src: "{{ installer_home }}/hive/hive-hbase-handler-3.1.2-v2.jar"
        dest: "{{ spark_home }}/jars/"

- name: Spark Preparations
  hosts: "{{ groups['spark'][0] }}"
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hadoop_home: "{{ bigdata_home }}/hadoop"
    spark_home: "{{ bigdata_home }}/spark"
  tasks:
    - name: Create Spark log directory
      command: "{{ hadoop_home }}/bin/hdfs dfs -mkdir -p /spark-logs"
    - name: Create Spark archive directory
      command: "{{ hadoop_home }}/bin/hdfs dfs -mkdir -p /spark/archive"
    - name: Upload Spark jars
      shell: "{{ hadoop_home }}/bin/hdfs dfs -put {{ spark_home }}/jars/* /spark/archive/"
    - name: Upload JDBC
      command: "{{ hadoop_home }}/bin/hdfs dfs -put {{ installer_home }}/jdbc/{{ item }} /spark/archive/"
      with_item:
        - "{{ installer.ojdbc }}"
        - "{{ installer.mysql_jdbc }}"
    - name: Upload hive-hbase-handler
      command: "{{ hadoop_home }}/bin/hdfs dfs -put {{ installer_home }}/hive/hive-hbase-handler-3.1.2-v2.jar /spark/archive/"

- name: Start history server
  hosts: spark-history
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    spark_home: "{{ bigdata_home }}/spark"
  tasks:
    - name: Start history server
      command: "{{ spark_home }}/sbin/start-history-server.sh"