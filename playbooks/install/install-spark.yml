---

- name: Install Spark
  hosts: spark
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    spark_home: "{{ bigdata_home }}/spark"
  tasks:
    - name: Create bigdata home directory
      become: yes
      file:
        dest: "{{ bigdata_home }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Copy installer
      copy:
        src: "{{ installer_home }}/{{ installer.spark }}"
        dest: "{{ bigdata_home }}/{{ installer.spark }}"
    - name: Unzip installer
      unarchive:
        src: "{{ bigdata_home }}/{{ installer.spark }}"
        dest: "{{ bigdata_home }}/"
        remote_src: true
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Delete installer
      file:
        path: "{{ bigdata_home }}/{{ installer.spark }}"
        state: absent
    - name: Create Spark symbolic link
      file:
        src: "{{ bigdata_home }}/{{ installer.spark.replace('.tgz', '') }}"
        dest: "{{ spark_home }}"
        state: link
    - name: Edit Spark home and pyspark
      lineinfile:
        path: "/home/{{ remote_user }}/.bashrc"
        line: "{{ item }}"
        insertafter: EOF
      with_items:
        - "export SPARK_HOME={{ spark_home }}"
        - "PATH=$PATH:$SPARK_HOME/bin"
    - name: Edit pyspark
      lineinfile:
        path: "/home/{{ remote_user }}/.bashrc"
        regexp: "^#? *{{ item.key | regex_escape() }}="
        line: "{{ item.key }}={{ item.value }}"
      with_dict:
        "export PYSPARK_PYTHON": "\"$HOME/.pyenv/shims/python\""
#        "export PYSPARK_DRIVER_PYTHON": python3
    - name: Add spark-env.sh
      template:
        src: spark/{{ item }}
        dest: "{{ spark_home }}/conf/"
        mode: 0644
      with_items:
        - spark-env.sh
        - spark-defaults.conf

- name: Setup Hive on Spark
  hosts: hive
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hive_home: "{{ bigdata_home }}/hive"
    spark_home: "{{ bigdata_home }}/spark"
  tasks:
    - name: Add spark-defaults.conf
      template:
        src: spark/spark-defaults.conf
        dest: "{{ hive_home }}/conf/"
        mode: 0644
    - name: Add Spark jars
      copy:
        src: "{{ item }}"
        dest: "{{ hive_home }}/lib/"
        mode: 0644
      with_fileglob:
        - "{{ installer_home }}/hive-on-spark/*_2.13*"
    - name: kryo test
      copy:
        src: "{{ installer_home }}/hive-on-spark/hive-kryo-registrator-4.0.0-alpha-1.jar"
        dest: "{{ hive_home }}/lib/"
        mode: 0644
    - name: kryo test
      file:
        path: "{{ hive_home }}/lib/hive-kryo-registrator-3.1.2.jar"
        state: absent

- name: Spark Preparations
  hosts: "{{ groups['spark'][0] }}"
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hadoop_home: "{{ bigdata_home }}/hadoop"
    spark_home: "{{ bigdata_home }}/spark"
  tasks:
    - name: Create Spark log directory
      command: "{{ hadoop_home }}/bin/hdfs dfs -mkdir -p /spark-logs"
    - name: Create Spark archive directory
      command: "{{ hadoop_home }}/bin/hdfs dfs -mkdir -p /spark/archive"
    - name: Upload Spark jars
      shell: "{{ hadoop_home }}/bin/hdfs dfs -put {{ spark_home }}/jars/* /spark/archive/"


# Hive, Spark 재시작 필요