---

- name: Install Spark
  hosts: bigdata, worker
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    spark_home: "{{ bigdata_home }}/spark"
  tasks:
    - name: Create bigdata home directory
      become: yes
      file:
        dest: "{{ bigdata_home }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Copy installer
      copy:
        src: "{{ installer_home }}/spark/{{ installer.spark }}"
        dest: "{{ bigdata_home }}/{{ installer.spark }}"
    - name: Unzip installer
      unarchive:
        src: "{{ bigdata_home }}/{{ installer.spark }}"
        dest: "{{ bigdata_home }}/"
        remote_src: true
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
        creates: "{{ spark_home }}"
    - name: Delete installer
      file:
        path: "{{ bigdata_home }}/{{ installer.spark }}"
        state: absent
    - name: Create Spark symbolic link
      file:
        src: "{{ bigdata_home }}/{{ installer.spark.replace('.tgz', '') }}"
        dest: "{{ spark_home }}"
        state: link
    - name: Edit Spark home and pyspark
      lineinfile:
        path: "/home/{{ remote_user }}/.bashrc"
        line: "{{ item }}"
        insertafter: EOF
      with_items:
        - "export SPARK_HOME={{ spark_home }}"
        - "PATH=$PATH:$SPARK_HOME/bin"
    - name: Edit pyspark
      lineinfile:
        path: "/home/{{ remote_user }}/.bashrc"
        regexp: "^#? *{{ item.key | regex_escape() }}="
        line: "{{ item.key }}={{ item.value }}"
      with_dict:
        "export PYSPARK_PYTHON": "\"$HOME/.pyenv/shims/python\""
#        "export PYSPARK_DRIVER_PYTHON": python3
    - name: Add spark-env.sh
      template:
        src: spark/{{ item }}
        dest: "{{ spark_home }}/conf/"
        mode: 0644
      with_items:
        - spark-env.sh
        - spark-defaults.conf
    - name: Hive configuration symbolic link
      file:
        src: "{{ bigdata_home }}/hive/conf/hive-site.xml"
        dest: "{{ spark_home }}/conf/hive-site.xml"
        state: link
    - name: Add hive-hbase-handler and hive-common
      copy:
        src: "{{ installer_home }}/hive/{{ item }}"
        dest: "{{ spark_home }}/jars/"
      with_items:
        - "hive-hbase-handler-3.1.2-v2.jar"
        - "hive-common-2.3.9-v2.jar"


- name: Spark Preparations
  hosts: "{{ groups['worker'][0] }}"
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hadoop_home: "{{ bigdata_home }}/hadoop"
    spark_home: "{{ bigdata_home }}/spark"
  tasks:
    - name: Create Spark log directory
      command: "{{ hadoop_home }}/bin/hdfs dfs -mkdir -p /spark-logs"
      register: slog_result
      failed_when: >
        ("File exists" not in slog_result.stderr) and
        (slog_result.stderr != "")
    - name: Create Spark archive directory
      command: "{{ hadoop_home }}/bin/hdfs dfs -mkdir -p /spark/archive"
      register: arc_result
      failed_when: >
        ("File exists" not in arc_result.stderr) and
        (arc_result.stderr != "")
    - name: Upload Spark jars
      shell: "{{ hadoop_home }}/bin/hdfs dfs -put {{ spark_home }}/jars/* /spark/archive/"
      register: upload_spark_jars
      failed_when: >
        ("File exists" not in upload_spark_jars.stderr) and
        (upload_spark_jars.stderr != "")
    - name: Upload JDBC
      command: "{{ hadoop_home }}/bin/hdfs dfs -put {{ installer_home }}/jdbc/{{ item }} /spark/archive/"
      with_items:
        - "{{ installer.ojdbc }}"
        - "{{ installer.mysql_jdbc }}"
      register: upload_jdbc
      failed_when: >
        ("File exists" not in upload_jdbc.stderr) and
        (upload_jdbc.stderr != "")
    - name: Upload hive-hbase-handler
      command: "{{ hadoop_home }}/bin/hdfs dfs -put {{ installer_home }}/hive/hive-hbase-handler-3.1.2-v2.jar /spark/archive/"
      register: upload_handler
      failed_when: >
        ("File exists" not in upload_handler.stderr) and
        (upload_handler.stderr != "")
    - name: Remove hive-common in HDFS
      command: "{{ hadoop_home }}/bin/hdfs dfs -rm /spark/archive/hive-common-2.3.9.jar"
      register: rm_result
      failed_when: >
        (
        "No such file or directory" not in rm_result.stderr
        and
        "fs.TrashPolicyDefault: Moved" not in rm_result.stderr
        )
        and
        (rm_result.stderr != "")
    - name: Upload hive-common
      command: "{{ hadoop_home }}/bin/hdfs dfs -put {{ installer_home }}/hive/hive-common-2.3.9-v2.jar /spark/archive/"
      register: upload_common
      failed_when: >
        (
        "File exists" not in upload_common.stderr
        )
        and
        (upload_common.stderr != "")


- name: Start History server
  hosts: spark-history
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    spark_home: "{{ bigdata_home }}/spark"
  tasks:
    - name: Check History server jps
      shell: "/usr/bin/ps -ef | grep -v grep | grep 'spark.deploy.history.HistoryServer'"
      register: hs_ps
      failed_when: false
    - name: Start History server
      command: "{{ spark_home }}/sbin/start-history-server.sh"
      when: '"spark.deploy.history.HistoryServer" not in hs_ps.stdout'