---

- name: Install HBase
  hosts: hbase
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hbase_home: "{{ bigdata_home }}/hbase"
  tasks:
    - name: Create bigdata home directory
      become: yes
      file:
        dest: "{{ bigdata_home }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Copy installer
      copy:
        src: "{{ installer_home }}/hbase/{{ installer.hbase }}"
        dest: "{{ bigdata_home }}/{{ installer.hbase }}"
    - name: Unzip installer
      unarchive:
        src: "{{ bigdata_home }}/{{ installer.hbase }}"
        dest: "{{ bigdata_home }}/"
        remote_src: true
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Delete installer
      file:
        path: "{{ bigdata_home }}/{{ installer.hbase }}"
        state: absent
    - name: Create hbase symbolic link
      file:
        src: "{{ bigdata_home }}/{{ installer.hbase.replace('-bin.tar.gz', '') }}"
        dest: "{{ hbase_home }}"
        state: link
    - name: Add hbase configuration
      template:
        src: "{{ item }}"
        dest: "{{ bigdata_home }}/hbase/conf/"
        mode: 0644
      with_fileglob:
        - "{{ ansible_home }}/playbooks/edit/templates/hbase/*"

- name: Upload HBase library on Spark archive
  hosts: "{{ groups['hbase'][0] }}"
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hadoop_home: "{{ bigdata_home }}/hadoop"
    hbase_home: "{{ bigdata_home }}/hbase"
  tasks:
    - name: Create Spark archive directory
      command: "{{ hadoop_home }}/bin/hdfs dfs -mkdir -p /spark/archive"
    - name: Upload HBase library
      shell: "{{ hadoop_home }}/bin/hdfs dfs -put {{ hbase_home }}/lib/hbase-* /spark/archive/"
      register: hb_result
      failed_when: >
        ("File exists" not in hb_result.stderr) and
        (hb_result.stderr != "")
    - name: Upload htrace library
      shell: "{{ hadoop_home }}/bin/hdfs dfs -put {{ hbase_home }}/lib/client-facing-thirdparty/* /spark/archive/"
      register: ht_result
      failed_when: >
        ("File exists" not in ht_result.stderr) and
        (ht_result.stderr != "")
    - name: Upload shaded-clients library
      shell: "{{ hadoop_home }}/bin/hdfs dfs -put {{ hbase_home }}/lib/shaded-clients/* /spark/archive/"
      register: sc_result
      failed_when: >
        ("File exists" not in sc_result.stderr) and
        (sc_result.stderr != "")


#    - name: |
#        Copy Hadoop 3 library]
#        [DESCRIPTION]
#        HBase 2.2.0 사용시 Hadoop 3.3.4 연동을 위해 라이브러리 제거
#        HBase 2.5.2 사용시 미적용
#        [DESCRIPTION
#      file:
#        path: "{{ hbase_home }}/lib/{{ item }}"
#        state: absent
#      with_items:
#        - hadoop-annotations-2.8.5.jar
#        - hadoop-auth-2.8.5.jar
#        - hadoop-client-2.8.5.jar
#        - hadoop-common-2.8.5.jar
#        - hadoop-common-2.8.5-tests.jar
#        - hadoop-distcp-2.8.5.jar
#        - hadoop-hdfs-2.8.5.jar
#        - hadoop-hdfs-client-2.8.5.jar
#        - hadoop-mapreduce-client-app-2.8.5.jar
#        - hadoop-mapreduce-client-common-2.8.5.jar
#        - hadoop-mapreduce-client-core-2.8.5.jar
#        - hadoop-mapreduce-client-hs-2.8.5.jar
#        - hadoop-mapreduce-client-jobclient-2.8.5.jar
#        - hadoop-mapreduce-client-shuffle-2.8.5.jar
#        - hadoop-minicluster-2.8.5.jar
#        - hadoop-yarn-api-2.8.5.jar
#        - hadoop-yarn-client-2.8.5.jar
#        - hadoop-yarn-common-2.8.5.jar
#        - hadoop-yarn-server-applicationhistoryservice-2.8.5.jar
#        - hadoop-yarn-server-common-2.8.5.jar
#        - hadoop-yarn-server-nodemanager-2.8.5.jar
#        - hadoop-yarn-server-resourcemanager-2.8.5.jar
#        - hadoop-yarn-server-tests-2.8.5-tests.jar
#        - hadoop-yarn-server-web-proxy-2.8.5.jar
#    - name: |
#        Copy Hadoop 3 library]
#        [DESCRIPTION]
#        HBase 2.2.0 사용시 Hadoop 3.3.4 연동을 위해 라이브러리 추가
#        HBase 2.5.2 사용시 미적용
#        [DESCRIPTION
#      file:
#        src: "{{ bigdata_home }}/hadoop/share/hadoop/{{ items }}"
#        dest: "{{ hbase_home }}/lib/"
#        state: link
#      with_items:
#        - hdfs/hadoop-hdfs-3.3.4.jar

- name: Start HBase all component
  hosts: "{{ groups['hbase-master'][0] }}"
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hbase_home: "{{ bigdata_home }}/hbase"
  tasks:
    - name: Start master process
      shell: "{{ hbase_home }}/bin/start-hbase.sh"
    - name: Start thrift server
      shell: "{{ hbase_home }}/bin/hbase-daemon.sh start thrift"


