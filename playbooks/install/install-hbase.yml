---

- name: Install HBase
  hosts: bigdata
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hbase_home: "{{ bigdata_home }}/hbase"
  tasks:
    - name: Create bigdata home directory
      become: yes
      file:
        dest: "{{ bigdata_home }}"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Copy installer
      copy:
        src: "{{ installer_home }}/hbase/{{ installer.hbase }}"
        dest: "{{ bigdata_home }}/{{ installer.hbase }}"
    - name: Unzip installer
      unarchive:
        src: "{{ bigdata_home }}/{{ installer.hbase }}"
        dest: "{{ bigdata_home }}/"
        remote_src: true
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
        creates: "{{ hbase_home }}"
    - name: Delete installer
      file:
        path: "{{ bigdata_home }}/{{ installer.hbase }}"
        state: absent
    - name: Create hbase symbolic link
      file:
        src: "{{ bigdata_home }}/{{ installer.hbase.replace('-bin.tar.gz', '') }}"
        dest: "{{ hbase_home }}"
        state: link
    - name: Add hbase configuration
      template:
        src: "hbase/{{ item }}"
        dest: "{{ bigdata_home }}/hbase/conf/"
        mode: 0644
      with_items:
        - "backup-masters"
        - "hbase-env.sh"
        - "hbase-site.xml"
        - "regionservers"
        - "hbase-jmx.yml"
    - name: Create tools directory
      file:
        dest: "{{ hbase_home }}/tools"
        state: directory
        owner: "{{ remote_user }}"
        group: "{{ remote_user }}"
    - name: Copy jmx_exporter
      copy:
        src: "{{ installer_home }}/prometheus/{{ installer.jmx_exporter }}"
        dest: "{{ hbase_home }}/tools/"


- name: Upload HBase library on Spark archive
  hosts: "{{ groups['hbase'][0] }}"
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hadoop_home: "{{ bigdata_home }}/hadoop"
    hbase_home: "{{ bigdata_home }}/hbase"
  tasks:
    - name: Create Spark archive directory
      command: "{{ hadoop_home }}/bin/hdfs dfs -mkdir -p /spark/archive"
      register: arc_result
      failed_when: >
        ("File exists" not in arc_result.stderr) and
        (arc_result.stderr != "")
    - name: Upload HBase library
      shell: "{{ hadoop_home }}/bin/hdfs dfs -put {{ hbase_home }}/lib/hbase-* /spark/archive/"
      register: hb_result
      failed_when: >
        ("File exists" not in hb_result.stderr) and
        (hb_result.stderr != "")
    - name: Upload htrace library
      shell: "{{ hadoop_home }}/bin/hdfs dfs -put {{ hbase_home }}/lib/client-facing-thirdparty/* /spark/archive/"
      register: ht_result
      failed_when: >
        ("File exists" not in ht_result.stderr) and
        (ht_result.stderr != "")
    - name: Upload shaded-clients library
      shell: "{{ hadoop_home }}/bin/hdfs dfs -put {{ hbase_home }}/lib/shaded-clients/* /spark/archive/"
      register: sc_result
      failed_when: >
        ("File exists" not in sc_result.stderr) and
        (sc_result.stderr != "")


#    - name: |
#        Copy Hadoop 3 library]
#        [DESCRIPTION]
#        HBase 2.2.0 사용시 Hadoop 3.3.4 연동을 위해 라이브러리 제거
#        HBase 2.5.2 사용시 미적용
#        [DESCRIPTION
#      file:
#        path: "{{ hbase_home }}/lib/{{ item }}"
#        state: absent
#      with_items:
#        - hadoop-annotations-2.8.5.jar
#        - hadoop-auth-2.8.5.jar
#        - hadoop-client-2.8.5.jar
#        - hadoop-common-2.8.5.jar
#        - hadoop-common-2.8.5-tests.jar
#        - hadoop-distcp-2.8.5.jar
#        - hadoop-hdfs-2.8.5.jar
#        - hadoop-hdfs-client-2.8.5.jar
#        - hadoop-mapreduce-client-app-2.8.5.jar
#        - hadoop-mapreduce-client-common-2.8.5.jar
#        - hadoop-mapreduce-client-core-2.8.5.jar
#        - hadoop-mapreduce-client-hs-2.8.5.jar
#        - hadoop-mapreduce-client-jobclient-2.8.5.jar
#        - hadoop-mapreduce-client-shuffle-2.8.5.jar
#        - hadoop-minicluster-2.8.5.jar
#        - hadoop-yarn-api-2.8.5.jar
#        - hadoop-yarn-client-2.8.5.jar
#        - hadoop-yarn-common-2.8.5.jar
#        - hadoop-yarn-server-applicationhistoryservice-2.8.5.jar
#        - hadoop-yarn-server-common-2.8.5.jar
#        - hadoop-yarn-server-nodemanager-2.8.5.jar
#        - hadoop-yarn-server-resourcemanager-2.8.5.jar
#        - hadoop-yarn-server-tests-2.8.5-tests.jar
#        - hadoop-yarn-server-web-proxy-2.8.5.jar
#    - name: |
#        Copy Hadoop 3 library]
#        [DESCRIPTION]
#        HBase 2.2.0 사용시 Hadoop 3.3.4 연동을 위해 라이브러리 추가
#        HBase 2.5.2 사용시 미적용
#        [DESCRIPTION
#      file:
#        src: "{{ bigdata_home }}/hadoop/share/hadoop/{{ items }}"
#        dest: "{{ hbase_home }}/lib/"
#        state: link
#      with_items:
#        - hdfs/hadoop-hdfs-3.3.4.jar

## 2023. 06. 20. 스크립트 동작 이상으로 인한 개별 중지로 변경
#- name: Start HBase all component
#  hosts: "{{ groups['hbase-master'][0] }}"
#  remote_user: "{{ remote_user }}"
#  become: no
#  vars:
#    hbase_home: "{{ bigdata_home }}/hbase"
#  tasks:
#    - name: Start master process
#      shell: "{{ hbase_home }}/bin/start-hbase.sh"

- name: Start HBase Master
  hosts: hbase-master
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hbase_home: "{{ bigdata_home }}/hbase"
  tasks:
    - name: Check HBase Master
      shell: "/usr/bin/jps | grep 'HMaster'"
      register: hm_result
      failed_when: false
    - name: Start HBase Master
      command: "{{ hbase_home }}/bin/hbase-daemon.sh start master"
      when: '"HMaster" not in hm_result'

- name: Start HBase RegionServer
  hosts: worker
  remote_user: "{{ remote_user }}"
  become: no
  vars:
    hbase_home: "{{ bigdata_home }}/hbase"
  tasks:
    - name: Check HBase RegionServer
      shell: "/usr/bin/jps | grep 'HRegionServer'"
      register: rs_result
      failed_when: false
    - name: Start HBase RegionServer
      command: "{{ hbase_home }}/bin/hbase-daemon.sh start regionserver"
      when: '"HRegionServer" not in rs_result'