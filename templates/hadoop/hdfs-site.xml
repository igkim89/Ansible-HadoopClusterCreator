<?xml version="1.0" encoding="UTF-8" ?>

<configuration>
    <property>
        <name>dfs.nameservices</name>
        <value>{{ hadoop.hdfs.nameservice }}</value>
    </property>
    <property>
        <name>dfs.ha.namenodes.{{ hadoop.hdfs.nameservice }}</name>
        <value>{% for nn in groups['namenode'] %}{% if nn != groups['namenode'][0] %},{% endif %}{{ nn }}{% endfor %}</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
<!--        <value>{{ hadoop.hdfs.namenode_dir }}</value>-->
        <value>{% for nd in hadoop.hdfs.namenode_dir %}{% if nd != hadoop.hdfs.namenode_dir[0] %},{% endif %}{{ nd }}{% endfor %}</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>{% for dd in hadoop.hdfs.datanode_dir %}{% if dd != hadoop.hdfs.datanode_dir[0] %},{% endif %}{{ dd }}{% endfor %}</value>
    </property>
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>{{ hadoop.hdfs.journalnode_dir }}</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.{{ hadoop.hdfs.nameservice }}.{{ groups['namenode'][0] }}</name>
        <value>{{ groups['namenode'][0] }}:{{ hadoop.hdfs.namenode_rpc_port }}</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.{{ hadoop.hdfs.nameservice }}.{{ groups['namenode'][1] }}</name>
        <value>{{ groups['namenode'][1] }}:{{ hadoop.hdfs.namenode_rpc_port }}</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.{{ hadoop.hdfs.nameservice }}.{{ groups['namenode'][0] }}</name>
        <value>{{ groups['namenode'][0] }}:{{ hadoop.hdfs.namenode_http_port }}</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.{{ hadoop.hdfs.nameservice }}.{{ groups['namenode'][1] }}</name>
        <value>{{ groups['namenode'][1] }}:{{ hadoop.hdfs.namenode_http_port }}</value>
    </property>
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://{% for jn in groups['journalnode'] %}{% if jn != groups['journalnode'][0] %};{% endif %}{{ jn }}:{{ hadoop.hdfs.journalnode_shared_edits_port }}{% endfor %}/{{ hadoop.hdfs.nameservice }}</value>
        <description>qjournal://igkim-01:8485;igkim-02:8485;igkim-03:8485/{{ hadoop.hdfs.nameservice }}</description>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>{{ hadoop.hdfs.webhdfs_enabled }}</value>
    </property>
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>shell(true)</value>
    </property>
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/home/{{ remote_user }}/.ssh/id_rsa</value>
    </property>
    <property>
        <name>dfs.client.failover.proxy.provider.{{ hadoop.hdfs.nameservice }}</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>{{ hadoop.hdfs.replication_factor }}</value>
    </property>
    <property>
        <name>dfs.blocksize</name>
        <value>{{ hadoop.hdfs.block_size }}</value>
    </property>
    <property>
        <name>dfs.client.use.datanode.hostname</name>
        <value>false</value>
    </property>
    <property>
        <name>fs.permissions.umask-mode</name>
        <value>{{ hadoop.hdfs.permission_umask }}</value>
    </property>
    <property>
        <name>dfs.namenode.acls.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.client.use.legacy.blockreader</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.client.read.shortcircuit</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.client.read.shortcircuit.skip.checksum</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.client.domain.socket.data.traffic</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.namenode.handler.count</name>
        <value>100</value>
    </property>
    <property>
        <name>dfs.namenode.service.handler.count</name>
        <value>63</value>
    </property>
    <property>
        <name>dfs.namenode.replication.work.multiplier.per.iteration</name>
        <value>{{ hadoop.hdfs.replication_work_multiplier_per_iteration }}</value>
    </property>
    <property>
        <name>dfs.namenode.replication.max-streams</name>
        <value>{{ hadoop.hdfs.replication_max_streams_soft }}</value>
    </property>
    <property>
        <name>dfs.namenode.replication.max-streams-hard-limit</name>
        <value>{{ hadoop.hdfs.replication_max_streams_hard }}</value>
    </property>
    <property>
        <name>dfs.namenode.stale.datanode.interval</name>
        <value>{{ hadoop.hdfs.stale_interval }}</value>
    </property>
    <property>
        <name>dfs.namenode.avoid.read.stale.datanode</name>
        <value>{{ hadoop.hdfs.avoid_read_stale }}</value>
    </property>
    <property>
        <name>dfs.namenode.avoid.write.stale.datanode</name>
        <value>{{ hadoop.hdfs.avoid_write_stale }}</value>
    </property>
    <property>
        <name>dfs.namenode.acls.enabled</name>
        <value>{{ hadoop.hdfs.acls_enabled }}</value>
    </property>
    <property>
        <name>dfs.datanode.max.transfer.threads</name>
        <value>4096</value>
    </property>
    <property>
        <name>dfs.datanode.failed.volumes.tolerated</name>
        <value>{{ hadoop.hdfs.datanode_failed_volumes_tolerated }}</value>
    </property>
    <property>
        <name>dfs.datanode.drop.cache.behind.reads</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.datanode.drop.cache.behind.writes</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.datanode.sync.behind.writes</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.datanode.readahead.bytes</name>
        <value>4194304</value>
    </property>
    <property>
        <name>dfs.datanode.max.locked.memory</name>
        <value>4294967296</value>
    </property>
    <property>
        <name>dfs.client.block.write.replace-datanode-on-failure.enable</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.hosts</name>
        <value>{{ bigdata_home }}/hadoop/conf/includes</value>
    </property>
    <property>
        <name>dfs.hosts.exclude</name>
        <value>{{ bigdata_home }}/hadoop/conf/excludes</value>
    </property>
</configuration>
